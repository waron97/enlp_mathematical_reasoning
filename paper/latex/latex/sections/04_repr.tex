\label{sec:repr}

The execution cycle illustrated above is meant to reflect fairly accurately the information provided about the functioning of MathPrompter in the original paper. Since it is of course not possible to provide all nuances in written form, a sitation naturally arises in which not all implementation details are clearly reported. The authors also did not publish any code, so such questions are left for the reproducers to explore. What follows here is a general overview of the gaps that this reproduction attempted fill in, which may differ from the original implementation.

Firstly, referencing step (1) from section \ref{sec:mp}, mapping Q to Qt involves extracting variables from the mathematical prompt - which can be a tricky task if the objective is a truly general solution. However, for the context of MultiArith, a simple regular expression targeting groups of digits was sufficient, although it is stil necessary to pay attention to edge cases such as the expression "mp3 player", where the script had initially identified a variable. Real-world implementations of MathPrompter should, however, dedicate more developer attention to handling edge cases in variable extraction. 

Moving on to steps (2) and (3), a point of uncertainty is represented by the fact that \citealt{imani-etal-2023-mathprompter} do not delineate precisely how the final prompts to the LLM are derived. In summary, if after evaluating the two completions for P1 and P2, the resulting expressions fail to converge to the same result given the random variables (e.g. in the case of "A + B - C" and "A - B / C"), it is unclear whether MathPrompter exits with failure or attempts to generate new completions by prompting the LLM again - and if so, how many re-prompts is it allowed per cycle. In this study, the instructions were taken to mean that in case of non-convergence, MathPrompter is allowed to request new completions from the LLM in step (3). For the purposes of this study, the limit to this has been set to 5 attempts - after 5 attempts, if no converging completion pair is found, the system declares no result for that cycle (this only concerns step (4), but remember that everything from step (2) though (4) is repeated an additional 5 times).

Another point of contention is the utilization of python's \emph{eval} function during result evaluation, which is accessed in both steps (3) and (4). The \emph{eval} method is a useful but not omnipotent tool to check code output - among other things, assignments and function definitions are not allowed. It's not stated to what extent the original implementation worked around these limitations. Particuarly problematic with regard to this are multiline python functions returned by the LLM: functions with only a return statement as their body are simple to map to a format that \emph{eval} can process, but multiple intermediate steps before a return statement are not. In that scenario, it would probably be more efficient to not rely on \emph{eval} at all and instead set up a proper execution environment for the function, but this would require significant changes to the experiment setting. As a consequence, this reproduction of the MathPrompter execution cycle ignored errors arising from functions consisting of more than a return statement, declaring non-convergence of completions in this scenario instead.

Lastly, it's worth mentioning that getting access to the MultiArith dataset is at the same time straightforward and difficult. When searching for MutliArith on a search engine, multiple results come up - in that sense, acquiring the data is in itself trivial. However, these sources of data do not come from the original publishers of the dataset, and there is no real proof of authenticity on them. Furthermore, the paper which is quoted in other research when attributing MultiArith \citep{roy-roth-2015-solving} contains no reference to this dataset in its body. Having an official release of MultiArith would be desirable for future work down the line.

For further clarification of the practical reproduction steps taken in this experiment, it should be sufficient to refer to the GitHub repository linked above. 

With only the aforementioned assumptions implemented, MathPrompter achieves 73.83\% accuracy on MultiArith, almost 20 percentage points less than the reported 92.5\%. This drop could be attributed to a number of factors, some of which are explored in section \ref{ref:reflection}, but here it is worth pointing out one that is fairly trivial to solve: integer division.

After manual review, it was observed that the prompts eliciting a Python function from the LLM often contained integer division (Pythons "//" operator), whereas the simple mathematical formulations of the solution did not. With the original variable values, this would not have caused an issue since all gold solutions are integers - however, with solution validation being carried out with random values, this difference often meant different outputs for completions that were identical save for integer division. It is possible that the original study did not have this issue (OpenAI models are known to be changed under the hood, even though both the original study and this reproduction explicitly used \emph{"text-davinci-003"}), or that it was regarded so minor that it did not get a mention in the paper.

By replacing integer division operators in the Python completions with simple division operators, MathPrompter is recorded to have an accuracy of 82.3\% in this experiment, which is closer to the originally reported value. 

A futher after-the-fact analysis was to check whether not requiring P1 and P2 to converge to the same prompt would make a difference in the results (previosuly, answers where both prompts failed to agree on a single value were simply discarded). The main motivation for this was that in a real-world scenario, some use cases could prefer showing a partial solution to the end user, even if confidence in its correctness was low. To confirm this, all unsolved questions left after accounting for integer division were re-examined by allowing a valid result even if only one of the two prompts lead to its computation. This, however, did not lead no any substantial change in the results: at most 1 or 2 questions benefited from this alteration across the runs. Crucially, this should have allowed the mathematical expression to provide a result when the Python solution involved multiple lines. Interestingly, this means that when the Python solution had intermediate steps (and was thus unprocessable), the mathematical expression from the other prompt was also wrong.