In recent years, breakthroughs in natural language processing (NLP) and language modelling in particular have enabled researchers and consumers to tackle mathematical problems through language models. It has become clear, however, that language models do not deal well mathematical problems in zero-shot contexts, even if those problems seem very easy to humans. Recently, a series of various research contributions has progressed the state of the art considerably, especially through the use of very heavy large language models (LLMs). Following these developments, \citealp{imani-etal-2023-mathprompter} reached quasi-SOTA performance with despite using relatively more limited resources. These authors' clever use of prompt engineering and result validation with simple computational resources are the target of this replication attempt. Code for this work can be found on GitHub\footnote{\url{https://github.com/waron97/enlp_mathematical_reasoning}}.